{
 "metadata": {
  "name": "",
  "signature": "sha256:910eafc4ea9ca59b5ffce566dcf7d3553e614c1355d8c3014aa5a52b3248b815"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preamble\n",
      "import os\n",
      "import codecs\n",
      "import csv\n",
      "import unicodedata\n",
      "from bs4 import BeautifulSoup\n",
      "import gzip\n",
      "from bs4 import SoupStrainer\n",
      "from collections import OrderedDict\n",
      "    import json\n",
      "import pandas as pd\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from pandas import DataFrame\n",
      "def jsonLoads(jsonString):\n",
      "    '''\n",
      "    # helper function to convert unicode to str\n",
      "    # used to decode SOAP reply\n",
      "    # http://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-ones-from-json-in-python\n",
      "\n",
      "    replace:\n",
      "        json.loads(result.PE_executeResponse.Response, object_hook=decode_dict)\n",
      "    with:\n",
      "        jsonLoads(result.PE_executeResponse.Response)\n",
      "    '''\n",
      "    def _decode_list(data):\n",
      "        rv = []\n",
      "        for item in data:\n",
      "            if isinstance(item, unicode):\n",
      "                item = item.encode('utf-8')\n",
      "            elif isinstance(item, list):\n",
      "                item = _decode_list(item)\n",
      "            elif isinstance(item, dict):\n",
      "                item = _decode_dict(item)\n",
      "            rv.append(item)\n",
      "        return rv\n",
      "\n",
      "    def _decode_dict(data):\n",
      "        rv = {}\n",
      "        for key, value in data.iteritems():\n",
      "            if isinstance(key, unicode):\n",
      "                key = key.encode('utf-8')\n",
      "            if isinstance(value, unicode):\n",
      "                value = value.encode('utf-8')\n",
      "            elif isinstance(value, list):\n",
      "                value = _decode_list(value)\n",
      "            elif isinstance(value, dict):\n",
      "                value = _decode_dict(value)\n",
      "            rv[key] = value\n",
      "        return rv\n",
      "    return json.loads(jsonString, object_hook=_decode_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'year': '2011', 'day': '15', 'month': '04'}\n",
        "{'medlineta': 'J Nucl Med', 'country': 'United States', 'nlmuniqueid': '0217410', 'issnlinking': '0161-5505'}\n",
        "[{'affiliation': 'Department of Neurology, Faculty of Medicine and University Clinic, Heinrich Heine University, Dusseldorf, Germany. martin.suedmeyer@uni-duesseldorf.de', 'initials': 'M', 'lastname': 'Sudmeyer', 'forename': 'Martin'}, {'affiliation': '', 'initials': 'C', 'lastname': 'Antke', 'forename': 'Christina'}, {'affiliation': '', 'initials': 'T', 'lastname': 'Zizek', 'forename': 'Tanja'}, {'affiliation': '', 'initials': 'M', 'lastname': 'Beu', 'forename': 'Markus'}, {'affiliation': '', 'initials': 'S', 'lastname': 'Nikolaus', 'forename': 'Susanne'}, {'affiliation': '', 'initials': 'L', 'lastname': 'Wojtecki', 'forename': 'Lars'}, {'affiliation': '', 'initials': 'A', 'lastname': 'Schnitzler', 'forename': 'Alfons'}, {'affiliation': '', 'initials': 'HW', 'lastname': 'Muller', 'forename': 'Hans-Wilhelm'}]\n",
        "medline14n0685.xml.gz:15\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['medline14n0682.txt']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def newAuthorOnlyRow(authorName, journalInfo ):\n",
      "    # journalInfo['authPosition'] = str(position) + ','+ str(total)\n",
      "    row = {'author': authorName, 'journals':[journalInfo] }\n",
      "    return row\n",
      "    \n",
      "\n",
      "def getAuthorInfo(d2, authorData, authorDF, count):\n",
      "\n",
      "\n",
      "    # get author list ([] if it doesn't exist)\n",
      "    if 'authors' in d2.keys():\n",
      "        authors = d2['authors']\n",
      "    else:\n",
      "        authors = []\n",
      "\n",
      "    # filter authors with no last name\n",
      "    authors = [author for author in authors if author['lastname']]\n",
      "\n",
      "\n",
      "    processedAuthor = []\n",
      "    for authPosition, author in enumerate(authors, start=1):\n",
      "        \n",
      "        journal = dict(d2['journal'])\n",
      "        journal['date'] = d2['date']\n",
      "\n",
      "        authorKey = (author['forename'] + author['lastname']).replace(\" \", \"\").lower()\n",
      "\n",
      "        if author['affiliation']:\n",
      "            journal['affiliation'] = author['affiliation']\n",
      "        else:\n",
      "            journal['affiliation'] = authors[0]['affiliation']\n",
      "\n",
      "        journal['authPosition'] = \"%d,%d\"%(authPosition,len(authors))\n",
      "\n",
      "        \n",
      "        # try:\n",
      "\n",
      "        if not author in processedAuthor: # check for duplicate author in article\n",
      "            if count == 0 or not authorData or authorDF[authorDF['author'] == authorKey].empty:\n",
      "                authorRow = {'author': authorKey, 'journals':[journal] }\n",
      "                authorData.append(authorRow)\n",
      "            else:\n",
      "                \n",
      "                authorData[authorDF[authorDF['author'] == authorKey].index]['journals'].append(journal)\n",
      "        else:\n",
      "            pass # skip duplicate author\n",
      "\n",
      "        # except:\n",
      "        #     print \"Exception: \", journal , \" :Author: \" , authorKey , \" :Count: \" , count\n",
      "        \n",
      "        processedAuthor.append(authorKey)\n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "def showProgress(counter, maxCount):\n",
      "    if counter%10000 == 0:\n",
      "        print \"done:\", float(counter)/float(maxCount) * 100.0\n",
      "\n",
      "    \n",
      "# limit lines read for testing\n",
      "testLineLimit = 1000\n",
      "\n",
      "for fileName in allFiles[:20]:\n",
      "    print \"Processing file: \" + fileName\n",
      "    f = open(infileDirPath+fileName, 'r')\n",
      "    data = f.readlines()\n",
      "    f.close\n",
      "    \n",
      "    data = data[:testLineLimit]\n",
      "    authorDF = []\n",
      "    authorData = []\n",
      "    subData = data\n",
      "    print \"Start: Processing \", len(subData) , \" records\"\n",
      "    for count, d2 in enumerate(subData):\n",
      "\n",
      "        if authorData:\n",
      "            authorDF = pd.DataFrame(authorData)\n",
      "        \n",
      "        getAuthorInfo(jsonLoads(d2), authorData, authorDF, count)\n",
      "\n",
      "        showProgress(count, len(subData))\n",
      "        \n",
      "    authorDFFinal = pd.DataFrame(authorData)\n",
      "    csvFileName = fileName.replace('.txt','.csv')\n",
      "    authorDFFinal.to_csv(outfileDirPath+csvFileName)\n",
      "\n",
      "    \n",
      "# quick test to compare the outputs (if true, all matches)\n",
      "xtest = DataFrame.from_csv(\"/vagrant/cs109_firstOrPerish/data_munging/authorOut/medline14n0682.csv\")\n",
      "np.all((xtest == master).journals)\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing file: medline14n0682.txt\n",
        "Start: Processing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000  records\n",
        "done: 0.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infileDirPath = \"/vagrant/cs109_firstOrPerish/data_munging/outfiles/\"\n",
      "# infileDirPath = \"../../../../../Desktop/outfile2_partial/outfile2/\"  # original\n",
      "allFiles = []\n",
      "for file in os.listdir(infileDirPath):\n",
      "    if file.endswith(\".txt\"):\n",
      "        allFiles.append(file)\n",
      "\n",
      "print allFiles[:20]\n",
      "\n",
      "\n",
      "outfileDirPath = \"/vagrant/cs109_firstOrPerish/data_munging/authorOut/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['medline14n0682.txt']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# limit lines read for testing\n",
      "testLineLimit = 1000\n",
      "\n",
      "for fileName in allFiles[:20]:\n",
      "    print \"Processing file: \" + fileName\n",
      "    f = open(infileDirPath+fileName, 'r')\n",
      "    data = f.readlines()\n",
      "    f.close\n",
      "    \n",
      "    data = data[:testLineLimit]\n",
      "    count = 0\n",
      "    authorDF = []\n",
      "    authorData = []\n",
      "    subData = data\n",
      "    print \"Start: Processing \", len(subData) , \" records\"\n",
      "    for d2 in subData:\n",
      "\n",
      "        if authorData:\n",
      "            authorDF = pd.DataFrame(authorData)\n",
      "        getAuthorInfo(jsonLoads(d2), authorData, authorDF, count) \n",
      "        count +=1\n",
      "        if count%10000 == 0:\n",
      "            print \"done:\", float(count)/float(len(subData)) * 100.0\n",
      "    authorDFFinal = pd.DataFrame(authorData)\n",
      "    csvFileName = fileName.replace('.txt','.csv')\n",
      "    authorDFFinal.to_csv(outfileDirPath+csvFileName)\n",
      "\n",
      "    \n",
      "# quick test to compare the outputs\n",
      "xtest = DataFrame.from_csv(\"/vagrant/cs109_firstOrPerish/data_munging/authorOut/medline14n0682.csv\")\n",
      "\n",
      "np.all((xtest == master).journals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing file: medline14n0682.txt\n",
        "Start: Processing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000  records\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import pandas as pd\n",
      "#authorDFFinal = pd.DataFrame(authorData)\n",
      "#print authorDFFinal.head(10)\n",
      "#print \"\\n\"\n",
      "#duplicate = authorData[0]\n",
      "#print duplicate\n",
      "#testJournals = authorDFFinal[authorDFFinal['author'] == 'jshildreth'].journals\n",
      "#The list of all journals are located below in the panda dataframe\n",
      "#print testJournals.tolist()[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#authorDFFinal.to_csv(\"../../../../../Desktop/authors/medline14n0685.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "master = DataFrame.from_csv(\"/vagrant/cs109_firstOrPerish/data_munging/authorOut/medline14n0682.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# quick test to compare the outputs\n",
      "xtest = DataFrame.from_csv(\"/vagrant/cs109_firstOrPerish/data_munging/authorOut/medline14n0682.csv\")\n",
      "\n",
      "np.all((xtest == master).journals)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def newAuthorOnlyRow(authorName, journalInfo ):\n",
      "#     # journalInfo['authPosition'] = str(position) + ','+ str(total)\n",
      "#     row = {'author': authorName, 'journals':[journalInfo] }\n",
      "#     return row\n",
      "    \n",
      "\n",
      "def getAuthorInfo(d2, authorData, count):\n",
      "\n",
      "\n",
      "    # get author list ([] if it doesn't exist)\n",
      "    if 'authors' in d2.keys():\n",
      "        authors = d2['authors']\n",
      "    else:\n",
      "        authors = []\n",
      "\n",
      "    # filter authors with no last name\n",
      "    authors = [author for author in authors if author['lastname']]\n",
      "\n",
      "\n",
      "    processedAuthor = []\n",
      "    for authPosition, author in enumerate(authors, start=1):\n",
      "        \n",
      "        article = {}\n",
      "\n",
      "        nlmuniqueid = d2['journal']['nlmuniqueid']\n",
      "        article_year = d2['date']['year']\n",
      "        \n",
      "        # journal['date'] = d2['date']\n",
      "\n",
      "        \n",
      "        \n",
      "        authorKey = (\".\".join([author['forename'].replace(\" \",\"_\"), author['lastname']])).lower()\n",
      "\n",
      "        if author['affiliation']:\n",
      "            affiliation = author['affiliation']\n",
      "        else:\n",
      "            affiliation = authors[0]['affiliation']\n",
      "\n",
      "        # authPosition = [authPosition,len(authors)]\n",
      "        authCount = len(authors)\n",
      "\n",
      "        article = dict(\n",
      "            nlmuniqueid  = nlmuniqueid,\n",
      "            affiliation  = affiliation,\n",
      "            authPosition = authPosition,\n",
      "            authCount    = authCount,\n",
      "            )\n",
      "\n",
      "        try:\n",
      "            if not authorKey in processedAuthor: # check for duplicate author in article\n",
      "                if authorKey not in authorData.keys():\n",
      "                    authorData[authorKey] = []\n",
      "                \n",
      "                authorData[authorKey].append(article)\n",
      "            else:\n",
      "                pass # skip duplicate author\n",
      "\n",
      "        except:\n",
      "            print \"Exception: \", article , \" :Author: \" , authorKey , \" :Count: \" , count\n",
      "        \n",
      "        processedAuthor.append(authorKey)\n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "def showProgress(counter, maxCount):\n",
      "    if counter%10000 == 0:\n",
      "        print \"done:\", float(counter)/float(maxCount) * 100.0\n",
      "\n",
      "    \n",
      "# limit lines read for testing\n",
      "testLineLimit = 2000\n",
      "\n",
      "for fileName in allFiles[:20]:\n",
      "    print \"Processing file: \" + fileName\n",
      "    data = open(infileDirPath+fileName).readlines()\n",
      "    \n",
      "    \n",
      "    if testLineLimit > 0:\n",
      "        data = data[:testLineLimit]\n",
      "    \n",
      "    authorData = {}\n",
      "        \n",
      "    print \"Start: Processing \", len(data) , \" records\"\n",
      "    for recordidx, d2 in enumerate(data):\n",
      "\n",
      "        getAuthorInfo(jsonLoads(d2), authorData, recordidx)\n",
      "\n",
      "        showProgress(recordidx, len(data))\n",
      "    \n",
      "    csvFilePath = outfileDirPath+fileName.replace('.txt','.csv')\n",
      "    \n",
      "    \n",
      "    outfile = open(csvFilePath, 'w')\n",
      "    writer = csv.writer(outfile, dialect=csv.excel)\n",
      "    for k,v in authorData.items():\n",
      "        writer.writerow( [k,v] )\n",
      "    outfile.close()\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing file: medline14n0682.txt\n",
        "Start: Processing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000  records\n",
        "done: 0.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Production Code\n",
      "The code below is a python script that is run independently from the notebook, using Knime to handle multi-processing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "##############\n",
      "# knime specific code\n",
      "pyOut = kIn\n",
      "outfilepath = kIn['outfilepath'][0]\n",
      "infilepath  = kIn['infilepath' ][0]\n",
      "##############\n",
      "\n",
      "\n",
      "\n",
      "import gzip, json, csv\n",
      "\n",
      "###################################################################\n",
      "def jsonLoads(jsonString):\n",
      "    '''\n",
      "    # helper function to convert unicode to str\n",
      "    # used to decode SOAP reply\n",
      "    # http://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-ones-from-json-in-python\n",
      "\n",
      "    replace:\n",
      "        json.loads(result.PE_executeResponse.Response, object_hook=decode_dict)\n",
      "    with:\n",
      "        jsonLoads(result.PE_executeResponse.Response)\n",
      "    '''\n",
      "    def _decode_list(data):\n",
      "        rv = []\n",
      "        for item in data:\n",
      "            if isinstance(item, unicode):\n",
      "                item = item.encode('utf-8')\n",
      "            elif isinstance(item, list):\n",
      "                item = _decode_list(item)\n",
      "            elif isinstance(item, dict):\n",
      "                item = _decode_dict(item)\n",
      "            rv.append(item)\n",
      "        return rv\n",
      "\n",
      "    def _decode_dict(data):\n",
      "        rv = {}\n",
      "        for key, value in data.iteritems():\n",
      "            if isinstance(key, unicode):\n",
      "                key = key.encode('utf-8')\n",
      "            if isinstance(value, unicode):\n",
      "                value = value.encode('utf-8')\n",
      "            elif isinstance(value, list):\n",
      "                value = _decode_list(value)\n",
      "            elif isinstance(value, dict):\n",
      "                value = _decode_dict(value)\n",
      "            rv[key] = value\n",
      "        return rv\n",
      "    return json.loads(jsonString, object_hook=_decode_dict)\n",
      "###################################################################\n",
      "\n",
      "\n",
      "def getAuthorInfo(d2, authorData, count):\n",
      "    # get author list ([] if it doesn't exist)\n",
      "    if 'authors' in d2.keys():\n",
      "        authors = d2['authors']\n",
      "    else:\n",
      "        authors = []\n",
      "\n",
      "    # filter authors with no last name\n",
      "    authors   = [author for author in authors if author['lastname']]\n",
      "    authCount = len(authors)\n",
      "\n",
      "    processedAuthor = []\n",
      "    for authPos, author in enumerate(authors, start=1):\n",
      "        \n",
      "        # author unique ID\n",
      "        authorKey = (\".\".join([author['forename'].replace(\" \",\"_\"), author['lastname']])).lower()\n",
      "\n",
      "        # assign first author affiliation if nothing else exists\n",
      "        if author['affiliation']:\n",
      "            affiliation = author['affiliation']\n",
      "        else:\n",
      "            affiliation = authors[0]['affiliation']\n",
      "\n",
      "\n",
      "        nlmuniqueid  = d2['journal']['nlmuniqueid']\n",
      "        year         = d2['date'   ]['year'       ]\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "        article = dict(\n",
      "            nlmuniqueid  = nlmuniqueid,\n",
      "            affiliation  = affiliation ,\n",
      "            authPos      = authPos,\n",
      "            authCount    = authCount,\n",
      "            year         = year\n",
      "            )\n",
      "\n",
      "        try:\n",
      "            if not authorKey in processedAuthor: # check for duplicate author in article\n",
      "                if authorKey not in authorData.keys():\n",
      "                    authorData[authorKey] = []\n",
      "                \n",
      "                authorData[authorKey].append(article)\n",
      "            else:\n",
      "                pass # skip duplicate author\n",
      "\n",
      "        except:\n",
      "            print \"Exception: \", article , \" :Author: \" , authorKey , \" :Count: \" , count\n",
      "        \n",
      "        processedAuthor.append(authorKey)\n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "# def showProgress(counter, maxCount):\n",
      "#     if counter%10000 == 0:\n",
      "#         print \"done:\", float(counter)/float(maxCount) * 100.0\n",
      "\n",
      "data = open(infilepath).readlines()\n",
      "\n",
      "\n",
      "# print \"Start: Processing \", len(data) , \" records\"\n",
      "authorData = {}\n",
      "for recordidx, d2 in enumerate(data):\n",
      "    try:\n",
      "        getAuthorInfo(jsonLoads(d2), authorData, recordidx)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "outfile = gzip.open(outfilepath, 'wb')\n",
      "writer = csv.writer(outfile, dialect=csv.excel)\n",
      "for k,v in authorData.items():\n",
      "    writer.writerow( [k,v] )\n",
      "outfile.close()\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}